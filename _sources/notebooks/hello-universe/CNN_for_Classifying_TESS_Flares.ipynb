{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9cc51250",
   "metadata": {},
   "source": [
    "# Classifying flaring stars with stella: a convolutional neural network for TESS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7f9875",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3669a91",
   "metadata": {},
   "source": [
    "## Learning Goals\n",
    "\n",
    "\n",
    "**In this tutorial, you will see an example of building, compiling, and training a CNN to classify astronomical data in vector form.**  By the end of this tutorial you will have a working example of a simple Convolutional Neural Network (CNN) in Keras. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a41d2af",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "CNNs are a class of machine learning (ML) algorithms that can extract information from data. In this notebook, you will walk through the basic steps of applying a CNN to data:\n",
    "1. Load the data and visualize a sample of the data\n",
    "2. Divide the data into training, validation, and testing sets.\n",
    "3. Build a CNN in Keras\n",
    "4. Compile the CNN\n",
    "5. Train the CNN to perform a classification task\n",
    "6. Evaluate the CNN performance on test data with a confusion matrix.\n",
    "7. Build a new, unlabeled dataset and apply the CNN.\n",
    "\n",
    "\n",
    "CNNs can be applied to a wide range of vector analysis tasks, including classification and regression.  Here, we will build, compile, and train CNN to classify whether a star has undergone a flaring event from its observed TESS light curve, and where the flaring events are located within the time series. This work is based on the model described in <a href='https://ui.adsabs.harvard.edu/abs/2020AJ....160..219F/abstract'>Feinstein et al. 2020</a> and presented in the <a href='https://ui.adsabs.harvard.edu/abs/2020JOSS....5.2347F/abstract'>`stella`</a> software package. \n",
    "\n",
    "**NOTE:** *The [`stella` team has publicly-available code and documentation](https://adina.feinste.in/stella/) for demonstrating the architecture and optimal performace of this model, which we encourage you to check out! The goal of this notebook is to step through the model building and training process.* "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f9813d",
   "metadata": {},
   "source": [
    "## Imports\n",
    "This notebook uses the following:\n",
    "- *numpy* to handle array functions\n",
    "- *astropy.io fits* for accessing FITS files\n",
    "- *matplotlib.pyplot* for plotting data\n",
    "- *stella* for generating the training set and processing data\n",
    "- *keras* for building the CNN\n",
    "- *sklearn* for model performance metrics\n",
    "- *lightkurve.search* for extracting light curves"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51f52c1",
   "metadata": {},
   "source": [
    "To download the `stella` package, see their [documentation](https://adina.feinste.in/stella/getting_started/installation.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f824294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# arrays\n",
    "import numpy as np\n",
    "\n",
    "# set random seed for reproducibility \n",
    "np.random.seed(42)\n",
    "\n",
    "# fits\n",
    "from astropy.io import fits\n",
    "from astropy.utils.data import download_file\n",
    "\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import ticker\n",
    "\n",
    "# stella CNN functions\n",
    "import stella\n",
    "\n",
    "# keras\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Input, Flatten, Dense, Activation, Dropout, BatchNormalization\n",
    "from keras.layers.convolutional import Convolution1D, MaxPooling1D\n",
    "\n",
    "# sklearn for performance metrics\n",
    "from sklearn import metrics\n",
    "\n",
    "# lightkurve\n",
    "from lightkurve.search import search_lightcurve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b0d6cf",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network (CNN)  for Vector Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006838fb",
   "metadata": {},
   "source": [
    "### 1. Download the training data using `stella`\n",
    "\n",
    "Load the sample of TESS lightcurves (input vectors) and flare classifications (output labels) to be used to train the CNN from the <a href='https://ui.adsabs.harvard.edu/abs/2020JOSS....5.2347F/abstract'>`stella`</a>. package.  `stella` pre-processes the light curves and splits the data into training, test and validation sets.\n",
    "\n",
    "The training set contains stars observed at 2-minute cadence in TESS Sectors 1 and 2, classified by hand and presented as a flare catalog by <a href='https://ui.adsabs.harvard.edu/abs/2020AJ....159...60G/abstract'>Gunther et al. 2020</a>. The light curves are processed into snippets of length 200 cadences, where each flaring event, if present, is located at the center of the snippet. The full sample of lightcurves contains 8694 positive classes (flare), and 35896 negative classes (no flare). For this notebook, we will download a subset of this sample. These data are described in  <a href='https://ui.adsabs.harvard.edu/abs/2020AJ....160..219F/abstract'>Feinstein et al. 2020</a>.\n",
    "\n",
    "The CNN will be used to predict the presence of flaring events as a function of observing cadence. The input to the CNN is a light curve (time, flux, and flux error) and the output is a \"probability light curve\", or probabilities (value between 0 and 1)  that the measurement at each cadence is of a flaring event (1=flare, 0=no flare). In other words, the CNN performs a **classification** task at each cadence. Before a probability light curve can be produced, `stella` first pre-processes the input light curve by assembling it into snippets of length 200 cadences to the model, so that it can predict a value for the flare probability at each valid candence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef26bca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: UnitsWarning: Unit 'Rsun' not supported by the VOUnit standard.  [astropy.units.format.vounit]\n",
      "WARNING: UnitsWarning: Unit 'Rsun' not supported by the VOUnit standard.  [astropy.units.format.vounit]\n",
      " 94%|████████████████████████████████████████▌  | 50/53 [02:44<00:14,  4.82s/it]"
     ]
    }
   ],
   "source": [
    "# set data directory\n",
    "data_dir = '.'\n",
    "\n",
    "# download flare catalog\n",
    "download = stella.DownloadSets(fn_dir=data_dir)\n",
    "download.download_catalog()\n",
    "# download light curve files\n",
    "download.flare_table = download.flare_table[0:500]\n",
    "download.download_lightcurves(remove_fits=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9d7807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build train, test, validation dataset, \"ds\"\n",
    "ds = stella.FlareDataSet(fn_dir=data_dir,\n",
    "                         catalog='Guenther_2020_flare_catalog.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5ac6a3",
   "metadata": {},
   "source": [
    "The `stella` dataset includes training, test and validation lightcurves (input vectors) and flare labels (output labels). `stella` applies the necessary pre-processing to the lightcurves for input to the CNN model. For more on how these are constructed, see the [Feinstein et al. 2020](https://ui.adsabs.harvard.edu/abs/2020AJ....160..219F/abstract). For our purpose (i.e., building the `stella` CNN from scratch to illustrate its structure and function), we first need to remove all lightcurves from the training, test and validation sets with NaN-valued inputs. To do this, we loop through the data and select only lightcurves without NaNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c382e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove lightcurves with NaNs from training, test and validation data\n",
    "def remove_nans(input_data):\n",
    "    '''Determine indices of files without NaNs'''\n",
    "\n",
    "    idx = []\n",
    "    for k in range(np.shape(input_data)[0]):\n",
    "        if len(input_data[k,:,:][np.isnan(input_data[k,:,:])])==0:\n",
    "            idx.append(k)\n",
    "    return idx\n",
    "\n",
    "# find indices in train, test and validation sets without NaNs\n",
    "idx_train = remove_nans(ds.train_data)\n",
    "idx_test = remove_nans(ds.test_data)\n",
    "idx_val = remove_nans(ds.val_data)\n",
    "\n",
    "ds.train_data = ds.train_data[idx_train]\n",
    "ds.train_labels = ds.train_labels[idx_train]\n",
    "\n",
    "ds.test_data = ds.test_data[idx_test]\n",
    "ds.test_labels = ds.test_labels[idx_test]\n",
    "\n",
    "ds.val_data = ds.val_data[idx_val]\n",
    "ds.val_labels = ds.val_labels[idx_val]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1673466",
   "metadata": {},
   "source": [
    "To visualize the structure of the lightcurves in the training set, we plot a random selection of 16 examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da95f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select random image indices:\n",
    "example_ids = np.random.choice(len(ds.train_labels),16)\n",
    "\n",
    "# pull the lightcurves and labels for these selections\n",
    "example_lightcurves = [ds.train_data[j] for j in example_ids]\n",
    "example_labels = [ds.train_labels[j] for j in example_ids]\n",
    "\n",
    "\n",
    "# initialize your figure\n",
    "fig1=plt.figure(figsize=(10,10))\n",
    "\n",
    "# loop through the randomly selected images and plot with labels\n",
    "for i in range(len(example_ids)):\n",
    "    plt.subplot(4, 4, i + 1)\n",
    "    if example_labels[i]==1: #Flares\n",
    "        plt.plot(example_lightcurves[i], 'r')\n",
    "        plt.title('Flare')\n",
    "    else: # Non-flares\n",
    "        plt.plot(example_lightcurves[i], 'k')\n",
    "        plt.title('Non-flare')\n",
    "    plt.xlabel('Cadences')\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aeda799",
   "metadata": {},
   "source": [
    "### 3. Build a CNN in Keras\n",
    "\n",
    "Here, we will build the CNN model described in [Feinstein et al. 2020](https://ui.adsabs.harvard.edu/abs/2020AJ....160..219F/abstract) and implemented in `stella` from scratch. \n",
    "\n",
    "Further details about Conv1D, MaxPooling1D, BatchNormalization, Dropout, and Dense layers can be found in the [Keras Layers Documentation](https://keras.io/api/layers/).  Further details about the sigmoid and softmax activation function can be found in the [Keras Activation Function Documentation](https://keras.io/api/layers/activations/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd9f4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# generate the model architecture\n",
    "# Written for Keras 2\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "seed = 2\n",
    "np.random.seed(seed)\n",
    "\n",
    "filter1 = 16\n",
    "filter2 = 64\n",
    "dense   = 32\n",
    "dropout = 0.1\n",
    "\n",
    "# Define architecture for model\n",
    "data_shape = np.shape(ds.train_data)\n",
    "input_shape = (np.shape(ds.train_data)[1], 1)\n",
    "\n",
    "x_in = Input(shape=input_shape)\n",
    "c0 = Convolution1D(7, filter1, activation='relu', padding='same', input_shape=input_shape)(x_in)\n",
    "b0 = MaxPooling1D(pool_size=2)(c0)\n",
    "d0 = Dropout(dropout)(b0)\n",
    "\n",
    "c1 = Convolution1D(3, filter2, activation='relu', padding='same')(d0)\n",
    "b1 = MaxPooling1D(pool_size=2)(c1)\n",
    "d1 = Dropout(dropout)(b1)\n",
    "\n",
    "\n",
    "f = Flatten()(d1)\n",
    "z0 = Dense(dense, activation='relu')(f)\n",
    "d2 = Dropout(dropout)(z0)\n",
    "y_out = Dense(1, activation='sigmoid')(d2)\n",
    "\n",
    "model = Model(inputs=x_in, outputs=y_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b7eaba",
   "metadata": {},
   "source": [
    "### 4. Compile the CNN\n",
    "\n",
    "Next, we compile the model.  As in [Feinstein et al. 2020](https://ui.adsabs.harvard.edu/abs/2020AJ....160..219F/abstract), we select the Adam opmimizer and the binary cross entropy loss function (as this is a binary classification problem). \n",
    "\n",
    "You can learn more about [optimizers](https://keras.io/api/optimizers/) and more about [loss functions for regression tasks](https://keras.io/api/losses/) in the [Keras documentation](https://keras.io/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9522437d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile Model\n",
    "optimizer = 'adam'\n",
    "fit_metrics = ['accuracy'] \n",
    "loss = 'binary_crossentropy'\n",
    "model.compile(loss=loss, optimizer=optimizer, metrics=fit_metrics)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9625444d",
   "metadata": {},
   "source": [
    "### 5. Train the CNN to perform a classification task\n",
    "\n",
    "We will start with training for 20 epochs, but this almost certainly won't be long enough to get great results.  Once you've run your model and evaluated the fit, you can come back here and run the next cell again for 100 epochs or longer.  \n",
    "\n",
    "You can learn more about `fit` [here](https://keras.rstudio.com/reference/fit.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044140f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_epoch = 20\n",
    "batch_size = 64\n",
    "shuffle = True\n",
    "\n",
    "# Train\n",
    "history = model.fit(ds.train_data, ds.train_labels,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=nb_epoch, \n",
    "                    validation_data=(ds.val_data, ds.val_labels), \n",
    "                    shuffle=shuffle,\n",
    "                    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5d7815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to file\n",
    "model_file = 'flare_model.h5'\n",
    "model.save(model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3fb3ba",
   "metadata": {},
   "source": [
    "### 6. Test the CNN performance \n",
    "\n",
    "Apply the CNN to predict flares on the \"test\" set, not used for training or validating the CNN, and evaluate the performance using a confusion matrix. See the documentation from [sklearn on confusion matrices](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html) for more information. The code for generating and plotting the confusion matrix below was adapted from the application by [Ciprijanovic et al. 2020](https://ui.adsabs.harvard.edu/abs/2020A%26C....3200390C/abstract) for [DeepMerge](https://github.com/deepskies/deepmerge-public)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40323c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(model, input_data, input_labels):\n",
    "    \n",
    "    # Compute flare predictions for the test dataset\n",
    "    predictions = model.predict(input_data)\n",
    "\n",
    "    # Convert to binary classification \n",
    "    predictions =  (predictions > 0.5).astype('int32') \n",
    "    \n",
    "    # Compute the confusion matrix by comparing the test labels (ds.test_labels) with the test predictions\n",
    "    cm = metrics.confusion_matrix(input_labels, predictions, labels=[0,1])\n",
    "    cm = cm.astype('float')\n",
    "\n",
    "    # Normalize the confusion matrix results. \n",
    "    cm_norm = cm / cm.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "    #plotting\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "\n",
    "    cax = ax.matshow(cm_norm)\n",
    "\n",
    "    plt.title('Confusion matrix (Pristine Images)', y=1.08)\n",
    "    \n",
    "    ax.set_xticks([0,1])\n",
    "    ax.set_xticklabels(['Flare', 'No Flare'])\n",
    "    \n",
    "    ax.set_yticks([0,1])\n",
    "    ax.set_yticklabels(['Flare', 'No Flare'])\n",
    "\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "\n",
    "    fmt = '.2f'\n",
    "    thresh = cm_norm.max() / 2.\n",
    "    for i in range(cm_norm.shape[0]):\n",
    "        for j in range(cm_norm.shape[1]):\n",
    "            ax.text(j, i, format(cm_norm[i, j], fmt),\n",
    "            ha=\"center\", va=\"center\",\n",
    "            color=\"white\" if cm_norm[i, j] < thresh else \"black\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ae9f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(model, ds.test_data, ds.test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06d9543",
   "metadata": {},
   "source": [
    "## FAQs\n",
    "\n",
    "- **The results don't look great... why?** From the confusion matrix in Section 6, when faced with the test dataset (i.e., data not used for training or validation), the model predicts a large fraction of false positive flare events, and consequently not enough true negative flare events). The published models from [Feinstein et al. 2020](https://ui.adsabs.harvard.edu/abs/2020JOSS....5.2347F/abstract) perform *much better*, and the confusion matrix **should look more like the results shown below**. We note that in this notebook we are using a subset of the available training data and training for only a subset of the optimal number of epochs for space and time considerations, but you are welcome to agument these restricitons, and as always check out [the `stella` repository](https://adina.feinste.in/stella/) for more information!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0d1153",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_url = 'https://archive.stsci.edu/hlsps/stella/hlsp_stella_tess_ensemblemodel_s042_tess_v0.1.0_cnn.h5'\n",
    "pretrained_model = load_model(download_file(file_url, cache=True, show_progress=True))\n",
    "\n",
    "plot_confusion_matrix(pretrained_model, ds.test_data, ds.test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f78555",
   "metadata": {},
   "source": [
    "- **Can I improve the model by changing it?** We only trained for 20 epochs, which is many fewer than the published model. Go back to Section 4 (\"Train the CNN to perform a classification task\") and increase the number of epochs to 100 (or more!) and train again. Does your model perform better? Your results may look better/worse/different from the published results due to the stochastic nature of training. \n",
    "\n",
    "\n",
    "- **Can I try a different model?  I think the results could be improved.** Yes!  You can try adding layers, swapping out the max pooling, changing the activation functions, swapping out the loss function, or trying a different optimizer or learning rate.  Experiment and see what model changes give the best results. You should be aware:  when you start training again, you pick up where your model left off.  If you want to \"reset\" your model to epoch 0 and random weights, you should run the cells to make and compile the model again.\n",
    "\n",
    "\n",
    "- **I want to test my model on my training data!** No. You will convince yourself that your results are much better than they actually are.  Always keep your training, validation, and testing sets completely separate!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8f00e3",
   "metadata": {},
   "source": [
    "## Extensions/Exercises\n",
    "\n",
    "- **Is the model \"overfitting\"?** Using the results of the model's `history` (saved as a result of the model training process), investigate the behavior of the training and validation losses and accuracies as a function of training epoch. Make a plot or two! How do the training and validation losses compare? How do the training and validation accuracies compare? If he loss for the validation set is higher than for the training set (and conversely, the accuracy for the validation set is lower than for the training set), the model may be suffering from [overfitting](https://www.tensorflow.org/tutorials/keras/overfit_and_underfit). \n",
    "\n",
    "\n",
    "\n",
    "- **Try applying this model to a new dataset** Using the built-in functionality provided by the `stella` package, you can pre-process your own 2-minute cadence TESS light curves and predict flares. An example workflow is shown below: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a5d5c7",
   "metadata": {},
   "source": [
    "### 7. Predict flares in a new dataset \n",
    "\n",
    "In this step, we will download light curves directly from TESS, pre-process them with `stella` for input to the CNN, and predict flares. The sample is a set of bright M dwarfs not featured in the training/validation/tests datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6897e825",
   "metadata": {},
   "outputs": [],
   "source": [
    "ticids = ['120461526', '278779899', '139754153', '273418879', '52121469', '188580272', '394015919', '183564259', '402104884']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2db6fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for all the selected targets, pull the available lightcurves using the lightkurve package\n",
    "lcs = []\n",
    "for name in ticids:\n",
    "    lc = search_lightcurve(target='TIC'+name, mission='TESS', sector=[1,2])\n",
    "    lc = lc.download_all()\n",
    "    lcs.append(lc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa4022a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the CNN using `stella`\n",
    "cnn = stella.ConvNN(output_dir=data_dir,ds=ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bdae03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(0, [20,12])\n",
    "\n",
    "for i, lc in enumerate(lcs):\n",
    "    # pull out on the first light curve in each set, if more than one exist\n",
    "    if len(lc)>0: lc = lc[0]\n",
    "        \n",
    "    # predict the flare probability light cuvey for the input data using `stella` \n",
    "    # (which applies the necessary pre-processing to the data for input to the CNN)\n",
    "    cnn.predict(model_file, times=lc.time.value, fluxes=lc.flux, errs=lc.flux_err)\n",
    "    \n",
    "    ax = fig.add_subplot(3,3,i+1)\n",
    "    im = ax.scatter(cnn.predict_time[0], cnn.predict_flux[0], c=cnn.predictions[0]) #, vmin=0, vmax=1)\n",
    "    \n",
    "    plt.colorbar(im, ax=ax, label='Probability of Flare')\n",
    "    ax.set_xlabel('Time [BJD-2457000]')\n",
    "    ax.set_ylabel('Normalized Flux')\n",
    "    ax.set_title('TIC {}'.format(lc.targetid));\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83eb6a9c",
   "metadata": {},
   "source": [
    "## About this Notebook\n",
    "\n",
    "**Author:** Claire Murray, Data Scientist, cmurray1@stsci.edu based on the <a href=\"https://ui.adsabs.harvard.edu/abs/2020JOSS....5.2347F/abstract\"> `stella`<\\a> software package for the CNN used in <a href=\"https://ui.adsabs.harvard.edu/abs/2020AJ....160..219F/abstract\">\"Flare Statistics for Young Stars from a Convolutional Neural Network Analysis of TESS Data\"</a>, Adina D. Feinstein et al. Astronomical Journal, Volume 160, Issue 5, November 2020, and the notebook \"CNN_for_cluster_masses\" by Michelle Ntampaka, Data Scientist, mntampaka@stsci.edu.\n",
    "\n",
    "**Updated On:** 2022-3-10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fdef93d",
   "metadata": {},
   "source": [
    "## Citations\n",
    "\n",
    "If you use this CNN, `stella`, `astropy`, or `keras` for published research, please cite the\n",
    "authors. Follow these links for more information:\n",
    "\n",
    "* [Citing the CNN](https://ui.adsabs.harvard.edu/abs/2020AJ....160..219F/abstract)\n",
    "* [Citing the `stella`](https://ui.adsabs.harvard.edu/abs/2020JOSS....5.2347F/abstract)\n",
    "* [Citing `astropy`](https://www.astropy.org/acknowledging.html)\n",
    "* [Citing `keras`](https://keras.io/getting_started/faq/#how-should-i-cite-keras)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52ded6f",
   "metadata": {},
   "source": [
    "[Top of Page](#top)\n",
    "<img style=\"float: right;\" src=\"https://raw.githubusercontent.com/spacetelescope/notebooks/master/assets/stsci_pri_combo_mark_horizonal_white_bkgd.png\" alt=\"Space Telescope Logo\" width=\"200px\"/> "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
